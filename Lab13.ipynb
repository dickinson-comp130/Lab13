{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab #13 - Stylometric Authorship Attribution of the Federalist Papers\n",
    "\n",
    "**COMP130 - Introduction to Computing**  \n",
    "**Dickinson College**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Federalist Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Federalist Papers](http://www.gutenberg.org/ebooks/18) is a collection of essays written in 1787 and 1788 arguing in support of the newly proposed constitution for the United States of America. The authorship of these pieces had been intended to be obscured from the outset, as they were published under the pseudonym _Publius_ (from [Publius Valerius Poplicola](https://en.wikipedia.org/wiki/Publius_Valerius_Publicola) or Publicola, a leader of the Roman revolution who's name means _friend of the people_). While the pseudonym was clever many, even at the time, knew the essays were written collectively by [Alexander Hamilton](https://www.biography.com/political-figure/alexander-hamilton), [James Madison](https://www.biography.com/us-president/james-madison), and [John Jay](https://www.biography.com/political-figure/john-jay). However, the exact authorship of twelve of the papers has been widely disputed.\n",
    "\n",
    "In this lab you will write code that uses some elementary _stylometric analysis_ techniques to attempt to attribute the authorship of the disputed Federalist Papers to Hamilton, Madison or Jay. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q1:__ The texts of the Federalist Papers are contained in the `papers` sub-directory in the `Lab13` folder.  How many Federalist Papers are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q2:__ What is the naming convention that is used to identify the papers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q3:__ Later in the lab it will be useful to generate a filename for a paper given its number.  Write a function that given an integer number returns the filename, including the relative path, to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q4:__ Write a function given the number of a paper, uses the function from __Q3:__ to get the filename, opens it, read the entire document into a `String`, closes the file, and returns the string containing the text.  Note: Check the `File` methods to find one that will read the entire file rather than just one line at a time.  Include a few statements that use your function to print out the first 100 characters of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stylometric Authorship Attribution\n",
    "\n",
    "Sytlometric authorship attribution is based on the idea that each author writes in their own style and that an author's _stylistic features_ will be consistent across their works. For example, some authors will use short words, while others will demonstrate a proclivity to employ more lengthy words.  Some will write in short sentences. While others will construct long, complicated sentences, with many phrases and clauses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q5:__ What other stylistic features can you think of that might be useful for stylometric authorship attribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, Stylometric analysis does a [distant reading](https://www.newyorker.com/books/page-turner/an-attempt-to-discover-the-laws-of-literature) of many works by the authors of concern, computing values for a number of stylistic features without attempting to understand its content. The collection of values of the chosen stylistic features create a _stylistic fingerprint_ for the author. The chosen stylistic features are then computed for a disputed work, creating a stylistic fingerprint for that piece of work.  This stylistic fingerprint is then compared to the stylistic fingerprints of the known authors and similarity measures are used to attribute authorship. \n",
    "\n",
    "Through the remainder of the lab you will write code that computes some stylistic features, combines them into a stylistic fingerprint, compares fingerprints and uses them for attribute authorship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Based Stylistic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were a number of _stylistic features_ mentioned above and we will use those and a few more in this lab.  The stylistic features we will use are described below.  The exercises introduce you to these features and have you write functions that will compute them for a piece of text. You will eventually use these features to compute stylistic fingerprints for Madison, Hamilton and Jay, which you'll then use to attribute the authorship of the disputed Federalist Papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Word Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q6:__ Write a function that given a `List` of words computes and returns the average length of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q7:__ Write a few automated tests for your function from __Q6:__.  These tests should pass a `List` of `String` values to the function and _assert_ that it correctly returns the average length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type-Token Ratio\n",
    "\n",
    "The [_type-token ratio_](https://carla.umn.edu/learnerlanguage/spn/comp/activity4.html) is the ratio of the number of distinct words in a text to the total number of words in that text.  Thus, the more distinct words in the text the larger the type-token ratio will be.  Thus, the type-token ratio is a measure of the size of the vocabulary that the author has used in the text. Then, as much as authors are prone to using their full vocabulary, the type-token ratio becomes a useful part of an author's stylistic fingerprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q8:__ Write a function that takes a `List` of words and returns the type-token ratio.  Hint: Use a dictionary of counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q9:__ Write a few automated tests that check that your function in __Q8__ works correctly.  These tests should pass a `List` of `String` values to the function and _assert_ that it correctly returns the type-token ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hapax Legomena Ratio\n",
    "\n",
    "A [_hapax legomenon_](https://educalingo.com/en/dic-en/hapax-legomena) \"is a word that occurs only once within a context, either in the written record of an entire language, in the works of an author, or in a single text.\" The _hapax legomena ratio_ is the ratio of words that occur exactly once in the text to the total number of words in the text. \n",
    "\n",
    "The [Wikipedia page for hapax legomenon](https://en.wikipedia.org/wiki/Hapax_legomenon) points to arguments both for and against the use of hapax legomenon as a stylistic feature. It cites P. N. Harrison, as arguing \"that the number of hapax legomena in [an] author's corpus indicates his or her vocabulary and is characteristic of the author as an individual.\" While then citing W. P. Workman in claiming it has \"limited value for author identification because it has \"considerable variation among works known to be by a single author, and disparate authors often show similar values.\"  \n",
    "\n",
    "For our purposes, the computation of the hapax legomena ratios is an interesting programming exercise, so we will keep it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q10:__ Write a function that takes a `List` of words and returns the hapax legomena ratio.  Hint: Use a dictionary of counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q11:__ Write a few automated tests that check that your function in __Q10__ works correctly.  These tests should pass a `List` of `String` values to the function and _assert_ that it correctly returns the hapax legomena ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q12:__ Both the type-token ratio and the hapax legomena ratio use a dictionary of counters.  Having both of these functions do the work of computing that dictionary for the same text would be inefficient.  Instead it would be better to compute the dictionary of counters once and pass it to each of the functions. In addition, having both of the functions contain repeated code for computing the dictionary of counters is not good program design.\n",
    "\n",
    "If necessary, improve the efficiency and design of your code by factoring the functionality of computing the dictionary of counters out into its own function.  Do this above in the cell for your answer to __Q8__.  This new function should takes in the `List` of words and return the dictionary of counters.  Then refactor your type-token ratio and hapax legomena ratio functions and test code in the cells above (__Q8__ through __Q11__) so that they take the dictionary of counters as an argument. Thus, the dictionary for a given piece of text would only need only be computed once.\n",
    "\n",
    "If you had already written your code in this way, well done! There is nothing for you to do for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Texts Into Words\n",
    "\n",
    "We now have some functions that compute useful stylometric features from a list of words. The challenge now is to break a real text document into a list of words.  We might imagine using the `split` method of the `String` class with a space as a token. But then things like sentences with commas, or semicolons; or other internal punctuation will result in words like 'commas,' or 'semicolons;'.  We could then add to our code to handle these cases.  In the process we would discover other cases like hyphenated-words and so on.\n",
    "\n",
    "Challenges like this are common in the field of [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing). Thus, it should not come as a surprise that there is a Python module that can help.  The [Natural Language Processing Tool Kit (NLTK)](https://www.nltk.org/) provides a wide variety of functions that encapsulate processes that make working with natural language much easier.  We will use the NLTK to break (i.e. _tokenize_) texts into words, sentences and sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q13:__ In order to use the NLTK you will need to import it and also download data that it uses to help identify words and sentences.  Run the code in the cell below to download this data.  Note: You will only need to do this one time on a given machine.  Thus, the `nltk.download` statements will not need to be in all of your programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q14:__ Do some on-line research to find a **simple example** that uses the NLTK `word_tokenize` function to split a `String` into a `List` of words. You should be able to find an example that is no more than a few lines and is understandable based on the things we have learned in this course. If what you find seems too complicated, keep looking.  Copy the example into the cell below and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q15:__ Adapt the example from __Q14__ to write a program that reads one of the Federalist Papers (see __Q4__), tokenizes it into a list of words and prints out the first 20 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q16:__ What does the `word_tokenize` function do with punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q17:__ Write a function that takes a `List` of words and filters out all of the elements that are not words. You'll have to decide what you want to be a non-word.  It might be useful to use your program from __Q15__ to help identify the types of non-words that show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q18:__ Write a few automated tests that check that your function in __Q17__ works correctly.  These tests should pass a `List` of `String` values to the function and check that the non-words (however you defined non-words) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q19:__ Just to get a glimpse into how this is all going to come together, write statements below that compute and print the average word length, type-token ratio and hapax legomena ratio for one of the Federalist Papers.  These features should be computed after the non-words have been excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Based Stylistic Features\n",
    "\n",
    "Above you worked out a number of word based stylistic features.  While individual words can tell us a lot about an author, we can learn more by considering sentence level features as well. In the next several questions you will develop functions that compute stylistic features about sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Sentence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q20:__ Write a function that takes a `List` of `String` values where each string is now a complete sentence, and returns the average number of words in the sentences. Hint: Use the `word_tokenizer` inside your function to break each sentence into words and then eliminate the non-words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q21:__ Write a few automated tests that check that your function in __Q20__ works correctly.  These tests should pass a `List` of `String` values to the function and _assert_ that it correctly returns the average sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q22:__  The complexity of a sentence is a little more difficult to specify precisely.  However, if we assume that more complex sentences contain more punctuation (commas, semicolons, colons, dashes, etc) then we can use that as a proxy for sentence complexity. Write a function that given a `List` of `String` values where each string is a complete sentence, and returns the average amount of punctuation in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q23:__ Write a few automated tests that check that your function in __Q22__ works correctly.  These tests should pass a `List` of `String` values to the function and _assert_ that it correctly returns the average amount of punctuation in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Sentences\n",
    "\n",
    "Like tokenizing text into words, tokenizing text into sentences is a complicated process with lots of special cases. We might at first imagine simply using sentence ending punctuation (period, question mark, etc.) as delimiters.  But sentences containing abbreviations (e.g. 'Dr. Braught wrote this question.') make it clear that this gets more complicated quickly. Like we did with words, we can use the more sophisticated functions in the NLTK to break a text into sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q24__: Do some on-line research to find a **simple example** that uses the NLTK `sent_tokenize` function to split a `String` into a `List` of sentences. You should be able to find an example that is no more than a few lines and is understandable based on the things we have learned in this course. If what you find seems too complicated, keep looking.  Copy the example into the cell below and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q25:__ Adapt the example from __Q24__ to write a program that reads one of the Federalist Papers (see __Q4__), tokenizes it into a list of sentences and prints out the first 7 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q26:__ Just to get another glimpse into how this is all going to come together, write statements below that compute and print the average sentence length and average sentence complexity for one of the Federalist Papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stylistic Fingerprints\n",
    "\n",
    "You now have a collection of functions that compute a variety of stylistic feature values for a given piece of text. These individual feature values can be combined into a _stylistic fingerprint_.  A stylistic fingerprint can be computed for an individual piece of text. A fingerprint for an author can be computed by averaging the fingerprints of each piece of work in their corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q27:__ Write a function that computes a stylistic fingerprint for one of the Federalist Papers given its number.  The fingerprint returned should be a list containing the values of the five stylistic features. Hint: Make use the functions that you defined above for:\n",
    "- Reading the file (__Q4__)\n",
    "- Computing the word length (__Q6__)\n",
    "- Computing the type-token ratio (__Q8__)\n",
    "- Computing the hapax legomena ration (__Q10__)\n",
    "- Computing the average sentence length (__Q20__)\n",
    "- Computing the avergee sentence complexity (__Q22__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q28:__ Write a function that computes a stylistic fingerprint for a given list of the Federalist Papers.  This function should accept a list of numbers indicating the papers, and return a fingerprint that is the average of the fingerprints for all of the papers.  Hint: Use the function from __Q27__ to compute the fingerprint for each paper, keep a _total fingerprint_ and then divide to find the _average fingerprint_ to be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q29:__ The lists below indicate the known authorship of the papers.  Write statements that use the lists below that uses your function from __Q28:__ to compute and display a fingerprint for each of the authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "jay = [2, 3, 4, 5, 64]\n",
    "madison = [10, 14, 18, 19, 20, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n",
    "hamilton = [1, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 21, 22, 23, 24,\n",
    "            25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 60,\n",
    "            61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,\n",
    "            78, 79, 80, 81, 82, 83, 84, 85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author Attribution\n",
    "\n",
    "So, who wrote the disputed papers? We are almost in a position to try to answer that question.  The last thing we will need is a way to compare the similarity of two stylistic fingerprints. We can then compare the fingerprint of a disputed document to the fingerprint of each author. The document can then be attributed to the author with the most similar fingerprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q30:__ Write a function that given two stylistic fingerprints computes and returns a value indicating the similarity of the fingerprints. For this question, the similarity of two signatures will be the sum of the _percent error_ between each of the stylistic feature values in the finger print. That is, this function should compute the percent error for each of the stylistic features and then add them together.  Thus, the smaller the value the more similar the fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q31:__ Write a function that given a paper number returns the name of the author (Madison, Hamilton or Jay) whose fingerprint (see __Q29__) most closely matches that of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q32:__ Write code below that uses your function from __Q31__ to display the attributed authorship for the papers written by John Jay.  We know these papers were written by Jay, so seeing how your program attributes them is an way to check how it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q33:__ Did your program get most of the papers written by Jay correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q34:__ Add code to the cell for __Q32__ that displays the authorship for the paper by Madison and Hamilton as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q35:__ Add code to the cell for __Q32__ that computes and displays the percentage of attributions that your program got correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q36:__ If the program were randomly guessing what percentage of the attributions would you expect it to get correct?  Did your program do better than if it were randomly guessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q37:__ The list below gives the numbers of the twelve Federalist Papers where the authorship had been disputed.  Write code that attributes authorship for each of these papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "disputed = [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 62, 63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q38:__ Do some research on-line.  To whom do most scholars now attribute the authorship of these disputed papers?  Give the URLs of any sites that you use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q39:__ Give an assessment of how well your program did at attributing authorship to the disputed papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Improvements\n",
    "\n",
    "It was not expected that our approach using the relatively simple stylometric features that we computed would do perfectly on this challenging problem.  But it should have done well enough to be convincing that the approach has potential.  Here are some thoughts on how it might be improved.  Feel free to add any of them to your programs.\n",
    "\n",
    "- __Filtering stop words:__ Stop words are simple words like 'a', 'an', 'the' and such that appear all through a text but have little to do with its actual meaning.  For _some_ stylometric features, they become more meaningful when the stop words are discarded. The NLTK has a way to get a `List` of stop words that you can then use to filter them out (sort of like we did with non-words).  Does filtering out stop words improve the performance? For which features should they be filtered out?\n",
    "- __Sections:__ The NLTK has a function that will tokenize a document based on the sections it contains.  This is a sophisticated method that does some analysis of the content to determine when the focus of the document shifts from one topic to another.  The number of sections or topics covered by a document can be a useful stylometric feature.  Does adding it to the fingerprint improve the performance?\n",
    "- __Weighted Fingerprints:__ Some of the features in a fingerprint have more discriminatory power than others. So counting those elements more (i.e. weighting them more heavily) in the comparison of fingerprints should improve the performance.  What weights should be used? How can you determine them? How much does it help?\n",
    "- __Other Similarity Measures:__ There are many other measures of similarity that could be used. Do measures other than total percent error (e.g. weighted distances or weighted averages) work better?\n",
    "- __Stemming:__ Stemming is the act of finding the root of a word (e.g. 'bus' and 'busses' are really the same thing).  So in computing values like the type-token ratio and the hapax legomena ratio using word roots might be an improvement over using the exact words. The NLTK provides functions that do stemming.  Does adding stemming to the computation of these features help the performance?\n",
    "- __N-Grams:__ An n-gram is a sequence of n words.  Most commonly stylometric approaches make use of 2-grams (i.e. bigrams).  This is done particularly with things like the type-token ratio and the hapax legomena ratio.  Does adding a bigram version of these features improve the accuracy of the author identification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The authorship of the disputed papers remained unsettled until the mid 1900's. Since then a combination of historical detective work and computer analysis has _largely_ settled the authorship. [Douglass Adair](https://en.wikipedia.org/wiki/Douglass_Adair) in his article [The Authorship of the Disputed Federalist Papers](https://www.jstor.org/stable/1921883?seq=1) attributed all of the disputed works to Madison. This attribution was reinforced by Mosteller and Wallace who used a computer to do _stylometric_ analysis as described in their 1964 book [Inference and Disputed Authorship: The Federalist](https://link.springer.com/chapter/10.1007/978-1-4612-5098-2_70).\n",
    "\n",
    "\n",
    "- Despite the above work, the site [www.congress.gov](https://www.congress.gov/), \"the official website for U.S. federal legislative information\" and \"provides access to accurate, timely, and complete legislative information for Members of Congress, legislative agencies, and the public\" still lists some of the [Federalist Papers](https://www.congress.gov/resources/display/content/The+Federalist+Papers) with shared and/or disputed authorship. \n",
    "\n",
    "\n",
    "- Stylometric approaches have been used in the real-world to\n",
    "  - make a convincing case that an [unattributed play was co-written by Shakespeare](http://content.time.com/time/arts/article/0,8599,1930971,00.html?artId=1930971?contType=article?chn=arts).\n",
    "  - do [fraud and plagarism detection](https://pdfs.semanticscholar.org/0df1/d4097c970483dac9b09bd65df1a2d397074b.pdf) for on-line learning systems.\n",
    "  - filter out and identify [spearfishing e-mails](https://seclab.ccs.neu.edu/static/publications/compsac2016profiler.pdf).\n",
    "  - identify the [authors of tweets](https://link.springer.com/chapter/10.1007/978-3-319-03689-2_3).\n",
    "  - [link multiple social media accounts owned by the same person](https://www.osti.gov/servlets/purl/1456316).\n",
    "  - provide evidence in [an immigration case](https://brooklynworks.brooklaw.edu/cgi/viewcontent.cgi?article=1043&context=jlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Acknowledgments:__ \n",
    "\n",
    "- This lab is adapted from Michelle Craig at the University of Toronto and her assignment [Authorship Detection](http://nifty.stanford.edu/2013/craig-authorship-detection/handout/index.shtml) as contained in the [Nifty Assignments](http://nifty.stanford.edu/) repository.  Using the Federalist papers as a compelling dataset was inspired by François Dominic Laramée's [\"Introduction to stylometry with Python\"](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python) in [The Programming Historian](https://programminghistorian.org/) #7 (2018), which uses the same data set with more sophisticated features than we used here techniques.\n",
    "- The text of the Federalist Papers was obtained from François Dominic Laramée's [\"Introduction to stylometry with Python\"](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python) which he split into the individual documents from the larger complete collection available on [Project Gutenberg](http://www.gutenberg.org/ebooks/18).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
